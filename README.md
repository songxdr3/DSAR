# DSAR
Code for paper "Distribution Shift Adaptive Reweighting for Robust Visual Question Answering"

# Abstract

Visual Question Answering (VQA) bridges vision and language to answer image-based questions, enhancing applications in healthcare, education, and human-computer interaction. VQA models often exhibit a strong language-prior bias, disproportionately relying on spurious question-answer correlations while neglecting critical visual information. In this paper, we reinterpret the language bias and vision bias issue in VQA through the lens of both label shift and covariate shift, proposing a novel \textbf{D}istribution \textbf{S}hift \textbf{A}daptive \textbf{R}eweighting (\textbf{DSAR}) framework to mitigate these challenges. Specifically, we introduce two complementary strategies: (1) a label-shift adaptation mechanism that dynamically reweights question categories based on their distributional characteristics, and (2) a feature-density-aware reweighting that adjusts sample weights based on feature sparsity patterns in the dataset. By integrating these adaptive reweighting mechanisms, we formulate an enhanced loss function that effectively mitigates language bias and improves model robustness, ensuring that performance improvements on out-of-distribution (OOD) data do not come at the cost of in-distribution (ID) performance. Extensive experiments on three baseline models (UpDn, SAN, and LXMERT) validate the effectiveness of our approach, achieving significant performance improvements on the challenging VQA-CP v1, VQA-CP v2, and VQA v2 benchmarks. The code will be released in the future.
